{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ce4f95-eb00-4028-805d-ad44068af7c7",
   "metadata": {},
   "source": [
    "# Oğuzhan Kır\n",
    "# 528231089"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b091a2-d677-435e-9a07-3de0c660501a",
   "metadata": {},
   "source": [
    "# Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960c2946-60e2-423b-bcff-89668b764c16",
   "metadata": {},
   "source": [
    "You are trying to find the best parking space to use that minimizes the time needed to get to your\n",
    "restaurant. There are 50 parking spaces, and you see spaces 1,2, . . . ,50 in order. As you approach each\n",
    "parking space, you see whether it is full or empty. We assume, somewhat heroically, that the probability\n",
    "that each space is occupied follows an independent Bernoulli process, which is to say that each space will\n",
    "be occupied with probability p, but will be free with probability 1 - p, and that each outcome is\n",
    "independent of the other.\n",
    "It takes 2 seconds to drive past each parking space and it takes 8 seconds to walk past. That is, if we park\n",
    "in space n, it will require 8(50 - n) seconds to walk to the restaurant. Furthermore, it would have taken\n",
    "you 2n seconds to get to this space. If you get to the last space without finding an opening, then you will\n",
    "have to drive into a special lot down the block, adding 30 seconds to your trip.\n",
    "We want to find an optimal strategy for accepting or rejecting a parking space.\n",
    "\n",
    "- (a) Give the sets of state and action spaces.\n",
    "- (b) Give the optimality equations for solving this problem.\n",
    "- (c) You have just looked at space 45, which was empty. There are five more spaces remaining (46 through\n",
    "50). What should you do? Using p = 0.6, find the optimal policy by solving your optimality equations for\n",
    "parking spaces 46 through 50.\n",
    "- d) Give the optimal value in part (c) corresponding to your optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3eceb6-eb50-4cbe-9ccb-a44f99014cd7",
   "metadata": {},
   "source": [
    "# States and action spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47273945-ce0f-4dc9-add7-4e80610f465d",
   "metadata": {},
   "source": [
    "State Space: The state can be represented by the number of the current parking space, i.e., $s \\in \\{1, 2, \\ldots, 52\\}$, where 51 represents special lot, 52 represents terminal state.\n",
    "\n",
    "Action Space: The action could be to park or move to the next parking space, i.e., $a \\in \\{P, C\\}$, where P represents parking and C represents continuing to the next parking spot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e3335e-fc90-4dc8-9b6a-2404a92edffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state and action spaces\n",
    "states = [i for i in range(1, 53)]\n",
    "action_space = {\"P\", \"C\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb324c-039d-448c-9720-ba8b2b994bd5",
   "metadata": {},
   "source": [
    "# Optimality equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a94e5-04a1-4e0b-aa3e-8cf731662b54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generic Bellman Optimality Equation\n",
    "\n",
    "The generic Bellman optimality equation for the value function $ V^*(s)$ of a state $ s $ is:\n",
    "\n",
    "$ V^*(s) = \\max_{a \\in A(s)} \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma V^*(s')] $\n",
    "\n",
    "where:\n",
    "- $ V^*(s) $  is the value of the optimal policy from state $s$.\n",
    "- $A(s)$ is the set of actions available in state $s$.\n",
    "- $P(s' \\mid s, a) $ is the probability of transitioning to state $ s' $ from state $ s $ under action $ a $.\n",
    "- $ R(s, a, s')$ is the reward received after taking action $ a $ in state $ s $ and transitioning to state $s'$.\n",
    "-$ \\gamma $ is the discount factor.\n",
    "  \n",
    "### Specialized Bellman Optimality Equation for the Parking Problem\n",
    "\n",
    "For the parking problem , the Bellman optimality equation can be specialized as follows:\n",
    "\n",
    "1. For $ s \\lt 50 $:\n",
    "$ V^*(s) = \\max \\left( -2 + p \\cdot V^*(s+1) + (1-p) \\cdot V^*(52), -2 + V^*(s+1) \\right) $\n",
    "\n",
    "2. For $ s = 50 $:\n",
    "$ V^*(50) = \\max \\left( -30, -2 + p \\cdot V^*(51) + (1-p) \\cdot V^*(52) \\right) $\n",
    "\n",
    "3. For $ s = 51$ and $s = 52 $:\n",
    "$ V^*(s) = 0 $\n",
    "\n",
    "where:\n",
    "- $ p $ is the probability that a parking space is occupied.\n",
    "- The term $-2$ represents the cost of driving past each parking space.\n",
    "- The term $-30$ represents the cost of driving to the special parking lot if no space is found before space 50.\n",
    "- $ V^*(s+1) $ represents the value of the next space.\n",
    "- $ V^*(51) $ represents the value of the special parking lot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914a9d8-af23-453b-ad99-20bcc797bf34",
   "metadata": {},
   "source": [
    "\n",
    "### Function Explanation\n",
    "\n",
    "#### `is_terminal(s)`\n",
    "Checks if a state $s$ is terminal, which means it's the end of the parking lot. It returns True if $s$ is 51 or 52, and False otherwise.\n",
    "\n",
    "#### `get_transition_probabilities()`\n",
    "Defines the transition probabilities for each state-action pair. For each state $s$, the transition probabilities are:\n",
    "- For each state $s$:\n",
    "  - If $s < 50$ and $s \\neq 45$:\n",
    "    - If choosing to continue driving (\"C\"), there is a 100% chance of moving to the next space ($s+1$): $P(s' = s+1 | s, \\text{\"C\"}) = 1$.\n",
    "    - If choosing to park (\"P\"), there is a 100% chance of moving to space 52 (terminal): $P(s' = 52 | s, \\text{\"P\"}) = 1$.\n",
    "  - If $s = 50$:\n",
    "    - If choosing to continue driving (\"C\"), there is a 100% chance of moving to the next space ($s+1$): $P(s' = s+1 | s=50, \\text{\"C\"}) = 1$.\n",
    "    - If choosing to park (\"P\"), there is a probability $p$ of moving to space 51 (special lot) and a probability of $1-p$ of moving to space 52 (terminal): $P(s' = 51 | s=50, \\text{\"P\"}) = p$, $P(s' = 52 | s=50, \\text{\"P\"}) = 1-p$.\n",
    "  - If $s = 45$:\n",
    "    - If choosing to continue driving (\"C\"), there is a 100% chance of moving to the next space ($s+1$): $P(s' = s+1 | s=45, \\text{\"C\"}) = 1$.\n",
    "    - If choosing to park (\"P\"), there is a 100% chance of moving to space 52 (terminal): $P(s' = 52 | s=45, \\text{\"P\"}) = 1$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### `get_rewards()`\n",
    "Defines the rewards for each state-action pair. For each state $s$, the rewards are:\n",
    "\n",
    "- If $s < 50$:\n",
    "  - If choosing to continue driving (\"C\") or parking (\"P\"), there is a penalty of -2 for each step: $R(s, \\text{\"C\"}, s+1) = R(s, \\text{\"P\"}, s+1) = -2$.\n",
    "- If parking at space 50, there is a penalty for parking and then walking to the restaurant but its equal to 0, and there is penalty for parking special lot:\n",
    "\n",
    "$R(50, \\text{\"P\"}, 52) = -8 \\times (50 - 50) = 0$\n",
    "\n",
    "$R(50, \\text{\"C\"}, 51) = -30$\n",
    "- If $s = 51$ or $s = 52$, there is no additional reward or penalty for parking: $R(51, \\text{\"P\"}, 52) = R(52, \\text{\"P\"}, 52) = 0$.\n",
    "\n",
    "#### `policy_evaluation(policy)`\n",
    "Evaluates the given policy to find the value function $V$. It iteratively updates the value of each state using the Bellman equation:\n",
    "\n",
    "$ V(s) = \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma V(s') $\n",
    "\n",
    "where $a$ is the action chosen according to the policy, $P(s' | s, a)$ is the transition probability, $R(s, a, s')$ is the reward, $V(s')$ is the value of the next state, and $\\gamma$ is the discount factor.\n",
    "\n",
    "#### `run_policy_iteration()`\n",
    "Runs the policy iteration algorithm to find the optimal policy. It iteratively evaluates the policy, updates the policy based on the current value function $V$, and checks for policy convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a7f6cc-c304-45a2-8718-c538c3c51785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 biggest_change: 154.8\n",
      "iter: 1 biggest_change: 154.8\n",
      "iter: 2 biggest_change: 142.0\n",
      "iter: 3 biggest_change: 87.60000000000001\n",
      "iter: 4 biggest_change: 58.80000000000001\n",
      "iter: 5 biggest_change: 51.12000000000003\n",
      "iter: 6 biggest_change: 46.80000000000001\n",
      "iter: 7 biggest_change: 28.080000000000013\n",
      "iter: 8 biggest_change: 21.168000000000006\n",
      "iter: 9 biggest_change: 14.832000000000008\n",
      "iter: 10 biggest_change: 14.832000000000008\n",
      "iter: 11 biggest_change: 12.700800000000015\n",
      "iter: 12 biggest_change: 12.700800000000015\n",
      "iter: 13 biggest_change: 7.620480000000015\n",
      "iter: 14 biggest_change: 4.572288000000015\n",
      "iter: 15 biggest_change: 3.203711999999996\n",
      "iter: 16 biggest_change: 3.203711999999996\n",
      "iter: 17 biggest_change: 3.203711999999996\n",
      "iter: 18 biggest_change: 1.9222271999999805\n",
      "iter: 19 biggest_change: 1.153336319999994\n",
      "iter: 20 biggest_change: 1.153336319999994\n",
      "iter: 21 biggest_change: 1.153336319999994\n",
      "iter: 22 biggest_change: 0.692001791999985\n",
      "iter: 23 biggest_change: 0.692001791999985\n",
      "iter: 24 biggest_change: 0.692001791999985\n",
      "iter: 25 biggest_change: 0.692001791999985\n",
      "iter: 26 biggest_change: 0.41520107519997396\n",
      "iter: 27 biggest_change: 0.249120645119973\n",
      "iter: 28 biggest_change: 0.1494723870720236\n",
      "iter: 29 biggest_change: 0.10883911679997027\n",
      "iter: 30 biggest_change: 0.10883911679997027\n",
      "iter: 31 biggest_change: 0.06530347007998216\n",
      "iter: 32 biggest_change: 0.039182082048000666\n",
      "iter: 33 biggest_change: 0.023509249228823137\n",
      "iter: 34 biggest_change: 0.019371621364541625\n",
      "iter: 35 biggest_change: 0.011622972818713606\n",
      "iter: 36 biggest_change: 0.006973783691250901\n",
      "iter: 37 biggest_change: 0.004184270214750541\n",
      "iter: 38 biggest_change: 0.003046798700040654\n",
      "iter: 39 biggest_change: 0.003046798700040654\n",
      "iter: 40 biggest_change: 0.0018280792200471296\n",
      "iter: 41 biggest_change: 0.0015063372773056471\n",
      "iter: 42 biggest_change: 0.000903802366394757\n",
      "iter: 0 biggest_change: 30.0\n",
      "iter: 1 biggest_change: 18.430597791743992\n",
      "iter: 2 biggest_change: 18.430597791743992\n",
      "iter: 3 biggest_change: 18.430597791743992\n",
      "iter: 4 biggest_change: 18.430597791743992\n",
      "iter: 5 biggest_change: 18.430597791743992\n",
      "iter: 6 biggest_change: 18.430597791743992\n",
      "iter: 7 biggest_change: 18.430597791743992\n",
      "iter: 8 biggest_change: 18.430597791743992\n",
      "iter: 9 biggest_change: 18.430597791743992\n",
      "iter: 10 biggest_change: 18.430597791743992\n",
      "iter: 11 biggest_change: 18.430597791743992\n",
      "iter: 12 biggest_change: 18.430597791743992\n",
      "iter: 13 biggest_change: 18.430597791743992\n",
      "iter: 14 biggest_change: 18.430597791743992\n",
      "iter: 15 biggest_change: 18.430597791743992\n",
      "iter: 16 biggest_change: 18.430597791743992\n",
      "iter: 17 biggest_change: 18.430597791743992\n",
      "iter: 18 biggest_change: 18.430597791743992\n",
      "iter: 19 biggest_change: 16.882397184000013\n",
      "iter: 20 biggest_change: 16.882397184000013\n",
      "iter: 21 biggest_change: 16.882397184000013\n",
      "iter: 22 biggest_change: 16.882397184000013\n",
      "iter: 23 biggest_change: 16.882397184000013\n",
      "iter: 24 biggest_change: 16.882397184000013\n",
      "iter: 25 biggest_change: 16.882397184000013\n",
      "iter: 26 biggest_change: 16.882397184000013\n",
      "iter: 27 biggest_change: 11.937024000000008\n",
      "iter: 28 biggest_change: 11.937024000000008\n",
      "iter: 29 biggest_change: 11.937024000000008\n",
      "iter: 30 biggest_change: 11.937024000000008\n",
      "iter: 31 biggest_change: 11.937024000000008\n",
      "iter: 32 biggest_change: 11.937024000000008\n",
      "iter: 33 biggest_change: 11.937024000000008\n",
      "iter: 34 biggest_change: 9.158400000000029\n",
      "iter: 35 biggest_change: 9.158400000000029\n",
      "iter: 36 biggest_change: 9.158400000000029\n",
      "iter: 37 biggest_change: 9.158400000000029\n",
      "iter: 38 biggest_change: 5.889024000000006\n",
      "iter: 39 biggest_change: 5.889024000000006\n",
      "iter: 40 biggest_change: 5.889024000000006\n",
      "iter: 41 biggest_change: 5.889024000000006\n",
      "iter: 42 biggest_change: 5.889024000000006\n",
      "iter: 43 biggest_change: 5.889024000000006\n",
      "iter: 44 biggest_change: 5.889024000000006\n",
      "iter: 45 biggest_change: 1.3996800000000036\n",
      "iter: 46 biggest_change: 1.3996800000000178\n",
      "iter: 47 biggest_change: 1.3996800000000178\n",
      "iter: 48 biggest_change: 1.3996800000000178\n",
      "iter: 49 biggest_change: 1.3996800000000178\n",
      "iter: 50 biggest_change: 0\n",
      "iter: 0 biggest_change: 10.022400000000005\n",
      "iter: 1 biggest_change: 10.022400000000005\n",
      "iter: 2 biggest_change: 10.022400000000005\n",
      "iter: 3 biggest_change: 10.022400000000005\n",
      "iter: 4 biggest_change: 10.022400000000005\n",
      "iter: 5 biggest_change: 10.022400000000005\n",
      "iter: 6 biggest_change: 10.022400000000005\n",
      "iter: 7 biggest_change: 10.022400000000005\n",
      "iter: 8 biggest_change: 10.022400000000005\n",
      "iter: 9 biggest_change: 10.022400000000005\n",
      "iter: 10 biggest_change: 10.022400000000005\n",
      "iter: 11 biggest_change: 10.022400000000005\n",
      "iter: 12 biggest_change: 10.022400000000005\n",
      "iter: 13 biggest_change: 10.022400000000005\n",
      "iter: 14 biggest_change: 10.022400000000005\n",
      "iter: 15 biggest_change: 10.022400000000005\n",
      "iter: 16 biggest_change: 10.022400000000005\n",
      "iter: 17 biggest_change: 10.022400000000005\n",
      "iter: 18 biggest_change: 10.022400000000005\n",
      "iter: 19 biggest_change: 10.022400000000005\n",
      "iter: 20 biggest_change: 10.022400000000005\n",
      "iter: 21 biggest_change: 10.022400000000005\n",
      "iter: 22 biggest_change: 10.022400000000005\n",
      "iter: 23 biggest_change: 10.022400000000005\n",
      "iter: 24 biggest_change: 10.022400000000005\n",
      "iter: 25 biggest_change: 10.022400000000005\n",
      "iter: 26 biggest_change: 10.022400000000005\n",
      "iter: 27 biggest_change: 10.022400000000005\n",
      "iter: 28 biggest_change: 10.022400000000005\n",
      "iter: 29 biggest_change: 10.022400000000005\n",
      "iter: 30 biggest_change: 10.022400000000005\n",
      "iter: 31 biggest_change: 10.022400000000005\n",
      "iter: 32 biggest_change: 10.022400000000005\n",
      "iter: 33 biggest_change: 10.022400000000005\n",
      "iter: 34 biggest_change: 10.022400000000005\n",
      "iter: 35 biggest_change: 10.022400000000005\n",
      "iter: 36 biggest_change: 10.022400000000005\n",
      "iter: 37 biggest_change: 10.022400000000005\n",
      "iter: 38 biggest_change: 10.022400000000005\n",
      "iter: 39 biggest_change: 10.022400000000005\n",
      "iter: 40 biggest_change: 10.022400000000005\n",
      "iter: 41 biggest_change: 10.022400000000005\n",
      "iter: 42 biggest_change: 10.022400000000005\n",
      "iter: 43 biggest_change: 10.022400000000005\n",
      "iter: 44 biggest_change: 4.7039999999999935\n",
      "iter: 45 biggest_change: 4.7039999999999935\n",
      "iter: 46 biggest_change: 4.7039999999999935\n",
      "iter: 47 biggest_change: 3.8400000000000034\n",
      "iter: 48 biggest_change: 2.4000000000000057\n",
      "iter: 49 biggest_change: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Probability that each space is occupied\n",
    "p = 0.6\n",
    "\n",
    "def is_terminal(s):\n",
    "    \"\"\"\n",
    "    Check if a state is terminal (end of the parking lot).\n",
    "    \"\"\"\n",
    "    return s in [51, 52]\n",
    "\n",
    "# Define transition probabilities for each state-action pair\n",
    "def get_transition_probabilities():\n",
    "    \"\"\"\n",
    "    Define transition probabilities for each state-action pair.\n",
    "    \"\"\"\n",
    "    transitions = {}\n",
    "    for s in states:\n",
    "        if s <= 50:\n",
    "            # If space is less than or equal to 50, there is a 100% chance of moving to the next space\n",
    "            transitions[(s, \"C\", s+1)] = 1\n",
    "            # If parking, there is a probability p of moving to the next space\n",
    "            transitions[(s, \"P\", s+1)] = p\n",
    "            # If parking, there is a probability of 1-p of moving to space 52 ( terminal)\n",
    "            transitions[(s, \"P\", 52)] = 1 - p\n",
    "        if s == 51:\n",
    "            # If in space 51, there is a 100% chance of moving to space 52 (terminal)\n",
    "            transitions[(s, \"P\", 52)] = 1\n",
    "        if s == 45:\n",
    "            # If in space 45, there is a 100% chance of moving to space 52 (terminal)\n",
    "            transitions[(s, \"C\", s+1)] = 1\n",
    "            transitions[(s, \"P\", 52)] = 1\n",
    "    return transitions\n",
    "\n",
    "# Define rewards for each state-action pair\n",
    "def get_rewards():\n",
    "    \"\"\"\n",
    "    Define rewards for each state-action pair.\n",
    "    \"\"\"\n",
    "    rewards = {}\n",
    "    for s in states:\n",
    "        if s < 50:\n",
    "            # If not at the end of the parking lot, there is a penalty of -2 for each step\n",
    "            rewards[(s, \"C\", s+1)] = -2\n",
    "            rewards[(s, \"P\", s+1)] = -2\n",
    "            # If parking, the reward is based on the distance to the restaurant\n",
    "            rewards[(s, \"P\", 52)] = -8*(50-s)\n",
    "        if s == 50:\n",
    "            # If in space 50, there is a penalty for parking special lot and then walking to the restaurant\n",
    "            rewards[(s, \"P\", 52)] = -8*(50-s)\n",
    "            rewards[(s, \"C\", s+1)] = -30\n",
    "            \n",
    "        if is_terminal(s):\n",
    "            # If in the special lot, there is no additional reward or penalty\n",
    "            rewards[(s, \"P\", 52)] = 0\n",
    "            \n",
    "    return rewards\n",
    "\n",
    "# Initialize transition probabilities and rewards\n",
    "transition_probs = get_transition_probabilities()\n",
    "rewards = get_rewards()\n",
    "\n",
    "# Initialize the value function\n",
    "V = {s: 0 for s in states}\n",
    "\n",
    "# Policy Evaluation\n",
    "def policy_evaluation(policy):\n",
    "    \"\"\"\n",
    "    Evaluate the given policy to find the value function V.\n",
    "    \"\"\"\n",
    "    gamma = 1\n",
    "    SMALL_ENOUGH = 1e-3\n",
    "    it = 0\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        for s in states:\n",
    "            if s in [51, 52]:\n",
    "                V[s] = 0\n",
    "            else:\n",
    "                old_v = V[s]\n",
    "                new_v = 0\n",
    "                a = policy[s]\n",
    "                for s2 in states:\n",
    "                    r = rewards.get((s, a, s2), 0)\n",
    "                    new_v += transition_probs.get((s, a, s2), 0) * (r + gamma * V[s2])\n",
    "                V[s] = new_v\n",
    "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "        print(\"iter:\", it, \"biggest_change:\", biggest_change)\n",
    "        it += 1\n",
    "        if biggest_change < SMALL_ENOUGH:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# Initialize a random policy\n",
    "policy = {s: np.random.choice([\"P\", \"C\"]) if s not in [51, 52] else \"P\" for s in states}\n",
    "\n",
    "# Policy Iteration\n",
    "def run_policy_iteration():\n",
    "    \"\"\"\n",
    "    Run the policy iteration algorithm to find the optimal policy.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        gamma = 1\n",
    "        V = policy_evaluation(policy)\n",
    "        is_policy_converged = True\n",
    "        for s in states:\n",
    "            if s not in [51, 52]:\n",
    "                old_a = policy[s]\n",
    "                best_value = float('-inf')\n",
    "                for a in action_space:\n",
    "                    v = 0\n",
    "                    for s2 in states:\n",
    "                        r = rewards.get((s, a, s2), 0)\n",
    "                        v += transition_probs.get((s, a, s2), 0) * (r + gamma * V[s2])\n",
    "                    if v > best_value:\n",
    "                        best_value = v\n",
    "                        policy[s] = a\n",
    "                if policy[s] != old_a:\n",
    "                    is_policy_converged = False\n",
    "        if is_policy_converged:\n",
    "            break\n",
    "\n",
    "# Run the policy iteration algorithm\n",
    "run_policy_iteration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e7465c0-7132-4738-83b4-9e2bec9adfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 'C', 2): -2,\n",
       " (1, 'P', 2): -2,\n",
       " (1, 'P', 52): -392,\n",
       " (2, 'C', 3): -2,\n",
       " (2, 'P', 3): -2,\n",
       " (2, 'P', 52): -384,\n",
       " (3, 'C', 4): -2,\n",
       " (3, 'P', 4): -2,\n",
       " (3, 'P', 52): -376,\n",
       " (4, 'C', 5): -2,\n",
       " (4, 'P', 5): -2,\n",
       " (4, 'P', 52): -368,\n",
       " (5, 'C', 6): -2,\n",
       " (5, 'P', 6): -2,\n",
       " (5, 'P', 52): -360,\n",
       " (6, 'C', 7): -2,\n",
       " (6, 'P', 7): -2,\n",
       " (6, 'P', 52): -352,\n",
       " (7, 'C', 8): -2,\n",
       " (7, 'P', 8): -2,\n",
       " (7, 'P', 52): -344,\n",
       " (8, 'C', 9): -2,\n",
       " (8, 'P', 9): -2,\n",
       " (8, 'P', 52): -336,\n",
       " (9, 'C', 10): -2,\n",
       " (9, 'P', 10): -2,\n",
       " (9, 'P', 52): -328,\n",
       " (10, 'C', 11): -2,\n",
       " (10, 'P', 11): -2,\n",
       " (10, 'P', 52): -320,\n",
       " (11, 'C', 12): -2,\n",
       " (11, 'P', 12): -2,\n",
       " (11, 'P', 52): -312,\n",
       " (12, 'C', 13): -2,\n",
       " (12, 'P', 13): -2,\n",
       " (12, 'P', 52): -304,\n",
       " (13, 'C', 14): -2,\n",
       " (13, 'P', 14): -2,\n",
       " (13, 'P', 52): -296,\n",
       " (14, 'C', 15): -2,\n",
       " (14, 'P', 15): -2,\n",
       " (14, 'P', 52): -288,\n",
       " (15, 'C', 16): -2,\n",
       " (15, 'P', 16): -2,\n",
       " (15, 'P', 52): -280,\n",
       " (16, 'C', 17): -2,\n",
       " (16, 'P', 17): -2,\n",
       " (16, 'P', 52): -272,\n",
       " (17, 'C', 18): -2,\n",
       " (17, 'P', 18): -2,\n",
       " (17, 'P', 52): -264,\n",
       " (18, 'C', 19): -2,\n",
       " (18, 'P', 19): -2,\n",
       " (18, 'P', 52): -256,\n",
       " (19, 'C', 20): -2,\n",
       " (19, 'P', 20): -2,\n",
       " (19, 'P', 52): -248,\n",
       " (20, 'C', 21): -2,\n",
       " (20, 'P', 21): -2,\n",
       " (20, 'P', 52): -240,\n",
       " (21, 'C', 22): -2,\n",
       " (21, 'P', 22): -2,\n",
       " (21, 'P', 52): -232,\n",
       " (22, 'C', 23): -2,\n",
       " (22, 'P', 23): -2,\n",
       " (22, 'P', 52): -224,\n",
       " (23, 'C', 24): -2,\n",
       " (23, 'P', 24): -2,\n",
       " (23, 'P', 52): -216,\n",
       " (24, 'C', 25): -2,\n",
       " (24, 'P', 25): -2,\n",
       " (24, 'P', 52): -208,\n",
       " (25, 'C', 26): -2,\n",
       " (25, 'P', 26): -2,\n",
       " (25, 'P', 52): -200,\n",
       " (26, 'C', 27): -2,\n",
       " (26, 'P', 27): -2,\n",
       " (26, 'P', 52): -192,\n",
       " (27, 'C', 28): -2,\n",
       " (27, 'P', 28): -2,\n",
       " (27, 'P', 52): -184,\n",
       " (28, 'C', 29): -2,\n",
       " (28, 'P', 29): -2,\n",
       " (28, 'P', 52): -176,\n",
       " (29, 'C', 30): -2,\n",
       " (29, 'P', 30): -2,\n",
       " (29, 'P', 52): -168,\n",
       " (30, 'C', 31): -2,\n",
       " (30, 'P', 31): -2,\n",
       " (30, 'P', 52): -160,\n",
       " (31, 'C', 32): -2,\n",
       " (31, 'P', 32): -2,\n",
       " (31, 'P', 52): -152,\n",
       " (32, 'C', 33): -2,\n",
       " (32, 'P', 33): -2,\n",
       " (32, 'P', 52): -144,\n",
       " (33, 'C', 34): -2,\n",
       " (33, 'P', 34): -2,\n",
       " (33, 'P', 52): -136,\n",
       " (34, 'C', 35): -2,\n",
       " (34, 'P', 35): -2,\n",
       " (34, 'P', 52): -128,\n",
       " (35, 'C', 36): -2,\n",
       " (35, 'P', 36): -2,\n",
       " (35, 'P', 52): -120,\n",
       " (36, 'C', 37): -2,\n",
       " (36, 'P', 37): -2,\n",
       " (36, 'P', 52): -112,\n",
       " (37, 'C', 38): -2,\n",
       " (37, 'P', 38): -2,\n",
       " (37, 'P', 52): -104,\n",
       " (38, 'C', 39): -2,\n",
       " (38, 'P', 39): -2,\n",
       " (38, 'P', 52): -96,\n",
       " (39, 'C', 40): -2,\n",
       " (39, 'P', 40): -2,\n",
       " (39, 'P', 52): -88,\n",
       " (40, 'C', 41): -2,\n",
       " (40, 'P', 41): -2,\n",
       " (40, 'P', 52): -80,\n",
       " (41, 'C', 42): -2,\n",
       " (41, 'P', 42): -2,\n",
       " (41, 'P', 52): -72,\n",
       " (42, 'C', 43): -2,\n",
       " (42, 'P', 43): -2,\n",
       " (42, 'P', 52): -64,\n",
       " (43, 'C', 44): -2,\n",
       " (43, 'P', 44): -2,\n",
       " (43, 'P', 52): -56,\n",
       " (44, 'C', 45): -2,\n",
       " (44, 'P', 45): -2,\n",
       " (44, 'P', 52): -48,\n",
       " (45, 'C', 46): -2,\n",
       " (45, 'P', 46): -2,\n",
       " (45, 'P', 52): -40,\n",
       " (46, 'C', 47): -2,\n",
       " (46, 'P', 47): -2,\n",
       " (46, 'P', 52): -32,\n",
       " (47, 'C', 48): -2,\n",
       " (47, 'P', 48): -2,\n",
       " (47, 'P', 52): -24,\n",
       " (48, 'C', 49): -2,\n",
       " (48, 'P', 49): -2,\n",
       " (48, 'P', 52): -16,\n",
       " (49, 'C', 50): -2,\n",
       " (49, 'P', 50): -2,\n",
       " (49, 'P', 52): -8,\n",
       " (50, 'P', 52): 0,\n",
       " (50, 'C', 51): -30,\n",
       " (51, 'P', 52): 0,\n",
       " (52, 'P', 52): 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75278e3d-4908-432d-bcbf-5179c7c7e4bb",
   "metadata": {},
   "source": [
    "## Optimal policy for all states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88301267-22f8-4007-9ed2-30ee970b44aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'C',\n",
       " 2: 'C',\n",
       " 3: 'C',\n",
       " 4: 'C',\n",
       " 5: 'C',\n",
       " 6: 'C',\n",
       " 7: 'C',\n",
       " 8: 'C',\n",
       " 9: 'C',\n",
       " 10: 'C',\n",
       " 11: 'C',\n",
       " 12: 'C',\n",
       " 13: 'C',\n",
       " 14: 'C',\n",
       " 15: 'C',\n",
       " 16: 'C',\n",
       " 17: 'C',\n",
       " 18: 'C',\n",
       " 19: 'C',\n",
       " 20: 'C',\n",
       " 21: 'C',\n",
       " 22: 'C',\n",
       " 23: 'C',\n",
       " 24: 'C',\n",
       " 25: 'C',\n",
       " 26: 'C',\n",
       " 27: 'C',\n",
       " 28: 'C',\n",
       " 29: 'C',\n",
       " 30: 'C',\n",
       " 31: 'C',\n",
       " 32: 'C',\n",
       " 33: 'C',\n",
       " 34: 'C',\n",
       " 35: 'C',\n",
       " 36: 'C',\n",
       " 37: 'C',\n",
       " 38: 'C',\n",
       " 39: 'C',\n",
       " 40: 'C',\n",
       " 41: 'C',\n",
       " 42: 'C',\n",
       " 43: 'C',\n",
       " 44: 'C',\n",
       " 45: 'C',\n",
       " 46: 'C',\n",
       " 47: 'C',\n",
       " 48: 'C',\n",
       " 49: 'C',\n",
       " 50: 'P',\n",
       " 51: 'P',\n",
       " 52: 'P'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1589e-b7f3-49f8-b942-ed6a1b04b056",
   "metadata": {},
   "source": [
    "## Optimal values for all states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa024b1a-5ffd-4da6-8c83-dcfcf1bb54d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: -98.0,\n",
       " 2: -96.0,\n",
       " 3: -94.0,\n",
       " 4: -92.0,\n",
       " 5: -90.0,\n",
       " 6: -88.0,\n",
       " 7: -86.0,\n",
       " 8: -84.0,\n",
       " 9: -82.0,\n",
       " 10: -80.0,\n",
       " 11: -78.0,\n",
       " 12: -76.0,\n",
       " 13: -74.0,\n",
       " 14: -72.0,\n",
       " 15: -70.0,\n",
       " 16: -68.0,\n",
       " 17: -66.0,\n",
       " 18: -64.0,\n",
       " 19: -62.0,\n",
       " 20: -60.0,\n",
       " 21: -58.0,\n",
       " 22: -56.0,\n",
       " 23: -54.0,\n",
       " 24: -52.0,\n",
       " 25: -50.0,\n",
       " 26: -48.0,\n",
       " 27: -46.0,\n",
       " 28: -44.0,\n",
       " 29: -42.0,\n",
       " 30: -40.0,\n",
       " 31: -38.0,\n",
       " 32: -36.0,\n",
       " 33: -34.0,\n",
       " 34: -32.0,\n",
       " 35: -30.0,\n",
       " 36: -28.0,\n",
       " 37: -26.0,\n",
       " 38: -24.0,\n",
       " 39: -22.0,\n",
       " 40: -20.0,\n",
       " 41: -18.0,\n",
       " 42: -16.0,\n",
       " 43: -14.0,\n",
       " 44: -12.0,\n",
       " 45: -10.0,\n",
       " 46: -8.0,\n",
       " 47: -6.0,\n",
       " 48: -4.0,\n",
       " 49: -2.0,\n",
       " 50: 0.0,\n",
       " 51: 0,\n",
       " 52: 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d27f3a-858d-48b2-9183-3ea72443c011",
   "metadata": {},
   "source": [
    "# Optimal policy for parking spaces 46 through 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16833358-0135-4174-ac30-e0a37d611e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(46, 'C'), (47, 'C'), (48, 'C'), (49, 'C'), (50, 'P')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(policy.items())[45:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec8213-252a-4f0f-b69f-dc8e688df539",
   "metadata": {},
   "source": [
    "# Optimal value for parking spaces 46 through 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd9edcb-1461-4024-bd03-b235cc04d259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(45, -10.0), (46, -8.0), (47, -6.0), (48, -4.0), (49, -2.0), (50, 0.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(V.items())[44:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b3642-fb29-4eea-a43b-779b31a56e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
